
#
# These sources are part of the "PyThon Programming Series" by Edgar Milvus, 
# you can find it on Amazon: https://www.amazon.com/dp/B0FTTQNXKG or
# https://tinyurl.com/PythonProgrammingSeries 
# New books info: https://linktr.ee/edgarmilvus 
#
# MIT License
# Copyright (c) 2025 Edgar Milvus
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# Source File: solution_exercise_1.py
# Description: Solution for Exercise 1
# ==========================================

import os
from google import genai
from google.genai import types
from typing import List, Dict, Tuple

# --- Configuration and Client Initialization ---
# Ensure your GEMINI_API_KEY is set in your environment variables
try:
    # The client automatically picks up the API key from the environment variable
    client = genai.Client()
except Exception as e:
    print(f"Error initializing client: {e}")
    print("Please ensure the GEMINI_API_KEY is configured correctly.")

# --- Common Tool Setup for Exercises 1, 2, and 3 ---
# Use the modern 'google_search' tool
GROUNDING_TOOL_MODERN = types.Tool(
    google_search=types.GoogleSearch()
)

CONFIG_MODERN = types.GenerateContentConfig(
    tools=[GROUNDING_TOOL_MODERN]
)

# --- Exercise 1: Real-Time Fact Check and Source Identification ---

print("--- Exercise 1: Real-Time Fact Check ---")

prompt_ex1 = "What was the final score of the most recent major cricket world cup final, and who played?"
model_ex1 = "gemini-2.5-flash"

try:
    response_ex1 = client.models.generate_content(
        model=model_ex1,
        contents=prompt_ex1,
        config=CONFIG_MODERN,
    )

    print(f"Prompt: {prompt_ex1}")
    print("\nModel Response Text (Grounded):")
    print(response_ex1.text)

    # 4. Access grounding metadata and print source titles
    metadata = response_ex1.candidates[0].grounding_metadata
    if metadata and metadata.grounding_chunks:
        print("\nSources Used (Titles):")
        source_titles = [chunk.web.title for chunk in metadata.grounding_chunks]
        # Use set to get unique titles, then convert back to list for printing
        unique_titles = list(set(source_titles))
        for i, title in enumerate(unique_titles):
            print(f"  {i+1}. {title}")
    else:
        print("\nNo grounding metadata found. Search tool may not have been utilized.")

except Exception as e:
    print(f"\nAn error occurred in Exercise 1: {e}")


# --- Exercise 2: Tracing the Model’s Reasoning Path ---

print("\n" + "="*50)
print("--- Exercise 2: Tracing the Model’s Reasoning Path ---")

prompt_ex2 = "Summarize the current economic outlook for the European Union's largest economy, and identify the name of its current finance minister."
model_ex2 = "gemini-2.5-flash"

try:
    response_ex2 = client.models.generate_content(
        model=model_ex2,
        contents=prompt_ex2,
        config=CONFIG_MODERN,
    )

    print(f"Prompt: {prompt_ex2}")
    print("\nModel Response Text:")
    print(response_ex2.text)

    # 3. Access webSearchQueries
    metadata_ex2 = response_ex2.candidates[0].grounding_metadata
    if metadata_ex2 and metadata_ex2.web_search_queries:
        print("\nModel's Generated Search Queries:")
        for i, query in enumerate(metadata_ex2.web_search_queries):
            print(f"  Query {i+1}: {query}")
    else:
        print("\nNo search queries were generated by the model.")

except Exception as e:
    print(f"\nAn error occurred in Exercise 2: {e}")


# --- Exercise 3: The Custom Citation Challenge ---

print("\n" + "="*50)
print("--- Exercise 3: The Custom Citation Challenge (Custom Citations) ---")

def generate_numbered_citations(response) -> Tuple[str, List[Dict[str, str]]]:
    """
    Modifies the response text to include simple bracketed citations [1]
    and returns a separate list of unique sources with URIs.
    """
    text = response.text
    try:
        candidate = response.candidates[0]
        if not candidate.grounding_metadata:
            return text, []

        supports = candidate.grounding_metadata.grounding_supports
        chunks = candidate.grounding_metadata.grounding_chunks

        # 1. Map chunks to unique sources and assign citation numbers
        # This dictionary maps the original chunk index (0, 1, 2...) to the new citation number (1, 2, 3...)
        chunk_to_citation_map: Dict[int, int] = {}
        citation_list: List[Dict[str, str]] = []
        current_citation_number = 1

        for original_index, chunk in enumerate(chunks):
            uri = chunk.web.uri
            title = chunk.web.title

            # Check if this URI/Title pair has already been added to our unique list
            is_new_source = True
            for citation_info in citation_list:
                if citation_info['uri'] == uri:
                    # Found a duplicate source. Map the original index to the existing citation number.
                    chunk_to_citation_map[original_index] = citation_info['number']
                    is_new_source = False
                    break

            if is_new_source:
                # This is a genuinely new source. Add it to the list.
                source_info = {
                    "number": current_citation_number,
                    "title": title,
                    "uri": uri
                }
                citation_list.append(source_info)
                chunk_to_citation_map[original_index] = current_citation_number
                current_citation_number += 1


        # 2. Sort supports by end_index in descending order.
        # CRITICAL: This prevents index shifting when inserting new text.
        sorted_supports = sorted(supports, key=lambda s: s.segment.end_index, reverse=True)

        # 3. Iterate and insert simple bracketed numbers
        for support in sorted_supports:
            end_index = support.segment.end_index
            if support.grounding_chunk_indices:
                # Get the unique citation numbers corresponding to the chunk indices
                citation_numbers = sorted(list(set(
                    chunk_to_citation_map.get(i)
                    for i in support.grounding_chunk_indices
                    if i in chunk_to_citation_map
                )))

                if citation_numbers:
                    # Format as [1], [2]
                    citation_string = ", ".join([f"[{num}]" for num in citation_numbers])
                    # Insert the citation string at the end_index
                    text = text[:end_index] + citation_string + text[end_index:]

        return text, citation_list

    except Exception as e:
        # In a real application, you might log this error instead of printing
        print(f"Error during citation generation: {e}")
        return text, []

# Generate a new grounded response for testing the function
try:
    response_ex3 = client.models.generate_content(
        model="gemini-2.5-flash",
        contents="Who won the latest season of the NBA championship and what was the team's home city?",
        config=CONFIG_MODERN,
    )

    final_text, sources = generate_numbered_citations(response_ex3)

    print(f"Original Text:\n{response_ex3.text}")
    print("\n--- Custom Citation Output ---")
    print(f"Modified Text:\n{final_text}")

    if sources:
        print("\nSource List:")
        for source in sources:
            print(f"  [{source['number']}]: {source['title']} ({source['uri']})")
    else:
        print("\nNo sources were processed.")

except Exception as e:
    print(f"\nAn error occurred in Exercise 3: {e}")


# --- Exercise 4: Exploring Legacy Grounding and Confidence Thresholds ---

print("\n" + "="*50)
print("--- Exercise 4: Legacy Grounding (Dynamic Retrieval) ---")

# 1. Define the legacy tool configuration (requires Gemini 1.5 model)
RETRIEVAL_TOOL_LEGACY = types.Tool(
    google_search_retrieval=types.GoogleSearchRetrieval(
        dynamic_retrieval_config=types.DynamicRetrievalConfig(
            mode=types.DynamicRetrievalConfigMode.MODE_DYNAMIC,
            # Set a high threshold (0.9) to make the model rely on internal knowledge
            dynamic_threshold=0.9
        )
    )
)

CONFIG_LEGACY = types.GenerateContentConfig(
    tools=[RETRIEVAL_TOOL_LEGACY]
)

prompt_ex4 = "What is the capital of France, and what is the approximate distance from the Eiffel Tower to the Louvre Museum?"
model_ex4 = 'gemini-1.5-flash' # Must use a 1.5 model for this legacy tool

try:
    print(f"Using Model: {model_ex4} with Dynamic Threshold: 0.9")

    response_ex4 = client.models.generate_content(
        model=model_ex4,
        contents=prompt_ex4,
        config=CONFIG_LEGACY,
    )

    print("\nModel Response Text:")
    print(response_ex4.text)

    # 5. Check for grounding metadata
    metadata_ex4 = response_ex4.candidates[0].grounding_metadata
    if metadata_ex4:
        print("\nGrounding Metadata FOUND. The model executed a search.")
        if metadata_ex4.web_search_queries:
             print(f"  Queries: {metadata_ex4.web_search_queries}")
    else:
        print("\nGrounding Metadata NOT FOUND. The model answered from its own knowledge (Confidence > 0.9).")

except Exception as e:
    print(f"\nAn error occurred in Exercise 4 (Legacy Tool): {e}")
